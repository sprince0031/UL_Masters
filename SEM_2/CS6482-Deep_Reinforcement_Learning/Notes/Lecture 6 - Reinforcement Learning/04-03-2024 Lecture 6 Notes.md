[Reference slides](https://learn.ul.ie/d2l/le/lessons/17967/topics/632154)
## Reinforcement Learning
- How do we compute the policy?
- How do we balance exploration vs exploitation?
### Elements in RL
- Policy
- Reward
- Value function

#### K armed bandit
- Look at the action selection methods and the outcome graphs (potential final exam question).

### Monte Carlo 
- Important when it comes to alpha go/fold/code

### Bellman's equation
- Look up and be able to explain (potential exam question).
- Draw the backup diagram for $q_{\pi}$ (potential exam question).
- **Bellman optimisation**
- Back up -

### RL Approaches
- Dynamic Programming
	- Backs up
	- requires a complete model
- Monte Carlo
	- model free
	- does not back up
- Temporal difference
	- model free
	- backs up
	- bridge between DP and Monte Carlo (best of both)

### Generalised Policy Iteration (GPI) - alg


## Dynamic Programming
### Policy Iteration
- Explain policy iteration. Slide #33 (potential exam question).
- Where in the following program does it indicate a model (high fidelity simulator) is required?
	- From slide #33
	- ![[Pasted image 20240304115650.png]]
### Value Iteration
