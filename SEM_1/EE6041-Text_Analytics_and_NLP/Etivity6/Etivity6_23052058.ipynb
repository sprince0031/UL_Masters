{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df8c917-7f53-4ac3-9297-f147cd5eb53b",
   "metadata": {},
   "source": [
    "## Student details\n",
    "Student name: **Siddharth Prince**  \n",
    "Student ID: **23052058**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5cb4e-8f8d-4759-9a3c-8b7b98317102",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e580ad9-b71c-4496-8c0d-87029091ace1",
   "metadata": {},
   "source": [
    "### a. Microaveraged Precision, Recall and F1 scores:\n",
    "For microaveraged scores, we sum up all the true positives, false positives and false negatives across all classes and then find out the precision, recall and F1 scores.  \n",
    "\n",
    "**Sum of true positives:**\n",
    "$$TP(Food) + TP(Drink) = 800 + 70 = 870$$  \n",
    "**Sum of false positives:**\n",
    "$$FP(Food) + FP(Drink) = 200 + 30 = 230$$\n",
    "**Sum of false negatives:**\n",
    "$$FN(Food) + FN(Drink) = 200 + 30 = 230$$  \n",
    "**Microaveraged precision:**  \n",
    "$$\\frac{True Positives}{True Positives + False Positives} = \\frac{870}{870 + 230} = 0.791$$  \n",
    "**Microaveraged recall:**  \n",
    "$$\\frac{True Positives}{True Positives + False Negatives} = \\frac{870}{870 + 230} = 0.791$$  \n",
    "**Microaveraged F1 score:**  \n",
    "$$2 * \\frac{microaveraged precision * microaveraged recall}{microaveraged precision + microaveraged recall} = 2 * \\frac{0.791 * 0.791}{0.791 + 0.791} = 0.791$$  \n",
    "In the above case, since precision and recall is the same, the denominator values just cancel out part of the numerator and leaves 0.791 again. Hence here, the microaveraged precision, recall and F1 scores are all equal!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57940f87-56dc-41e7-93b4-62a159906058",
   "metadata": {},
   "source": [
    "### b. Macroaveraged Precision, Recall and F1 scores:\n",
    "For macroaveraged scores, we find the true positives, false positives and false negatives for all classes individually and then find the averages of theses scores.  \n",
    "\n",
    "**Precision score for the \"Food\" class:**  \n",
    "$$\\frac{TP(Food)}{TP(Food) + FP(Food)} = \\frac{800}{800 + 200} = 0.8$$  \n",
    "**Precision score for the \"Drink\" class:**  \n",
    "$$\\frac{TP(Drink)}{TP(Drink) + FP(Drink)} = \\frac{70}{70 + 30} = 0.7$$  \n",
    "**Macroaveraged precision:**  \n",
    "$$\\frac{precision(Food) + precision(Drink)}{n(classes)} = \\frac{0.8 + 0.7}{2} = 0.75$$  \n",
    "**Recall score for the \"Food\" class:**  \n",
    "$$\\frac{TP(Food)}{TP(Food) + FN(Food)} = \\frac{800}{800 + 200} = 0.8$$  \n",
    "**Recall score for the \"Drink\" class:**  \n",
    "$$\\frac{TP(Drink)}{TP(Drink) + FN(Drink)} = \\frac{70}{70 + 30} = 0.7$$  \n",
    "**Macroaveraged recall:**  \n",
    "$$\\frac{recall(Food) + recall(Drink)}{n(classes)} = \\frac{0.8 + 0.7}{2} = 0.75$$  \n",
    "**F1 score for the \"Food\" class:**  \n",
    "$$2 * \\frac{precision(Food) * recall(Food)}{precision(Food) + recall(Food)} = 2 * \\frac{0.8 * 0.8}{0.8 + 0.8} = 0.8$$  \n",
    "**F1 score for the \"Drink\" class:**  \n",
    "$$2 * \\frac{precision(Drink) * recall(Drink)}{precision(Drink) + recall(Drink)} = 2 * \\frac{0.7 * 0.7}{0.7 + 0.7} = 0.7$$  \n",
    "**Macroaveraged F1 score:**  \n",
    "$$\\frac{F1(Food) + F1(Drink)}{n(classes)} = \\frac{0.8 + 0.7}{2} = 0.75$$  \n",
    "Again, since the false positives and false negatives are all the same for either of the classes, all the scores, including the F1 scores work out to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94527c4-1699-43d0-8792-56865e402c10",
   "metadata": {},
   "source": [
    "### Reason for difference between the micro and macroaveraged F1 scores:\n",
    "In the case of microaveraged F1 score, since we find the precision and recall scores for all the classes combined, the score will tend to skew more towards the class with a higher score. But in the case of macroaveraged F1 score, we find the average of the individual F1 scores of each class. Hence, this will be more towards the middle of both the scores. This is why we've gotten 0.75 for the macroaveraged case as opposed to 0.791 in the microaveraged case (the food class has a higher F1 score and hence it skews more toward this direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3439a-f2ca-4cc4-ab7a-fb81845f84eb",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e898141-db82-40d1-b7b0-3ea946f8a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util methods\n",
    "def getBagOfWords(wordList):\n",
    "    bagOfWords = {}\n",
    "    for word in wordList:\n",
    "        if word in bagOfWords:\n",
    "            bagOfWords[word] += 1\n",
    "        else:\n",
    "            bagOfWords[word] = 1\n",
    "    return bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5eef83-01e8-43e8-97d0-6391b3de064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to self: Make the function more general where it can take in any nnumber of classes and process the outputs accordingly.\n",
    "def naiveBayesClassifier(trainingSet, testingSet):\n",
    "    # Creating a \"mega doc\" string of all the strings in the training doc set corresponding to each class\n",
    "    megaDocGB = ' '.join([doc[0].lower() for doc in trainingSet if doc[1] == '+'])\n",
    "    megaDocIE = ' '.join([doc[0].lower() for doc in trainingSet if doc[1] == '-'])\n",
    "\n",
    "    print(f'megaDoc(+ve) = {megaDocGB}')\n",
    "    print(f'megaDoc(-ve) = {megaDocIE}\\n')\n",
    "    \n",
    "    megDocGBList = megaDocGB.split()\n",
    "    megDocIEList = megaDocIE.split()\n",
    "\n",
    "    # Getting the doc count for each class to calculate the priors\n",
    "    docCountGB = len([doc[1] for doc in trainingSet if doc[1] == '+'])\n",
    "    docCountIE = len([doc[1] for doc in trainingSet if doc[1] == '-'])\n",
    "\n",
    "    # Calculating the priors\n",
    "    probGB = docCountGB/len(trainingSet)\n",
    "    probIE = docCountIE/len(trainingSet)\n",
    "    print(f'prob(+ve) = {probGB}\\tprob(-ve) = {probIE}\\n')\n",
    "\n",
    "    '''\n",
    "    Using the util function that counts the occurance of each word in a list of strings.\n",
    "    It returns a dictionary with counts for each string.\n",
    "    '''\n",
    "    GB_BoW = getBagOfWords(megDocGBList)\n",
    "    IE_BoW = getBagOfWords(megDocIEList)\n",
    "    print(f'+ve_BoW = {GB_BoW}')\n",
    "    print(f'-ve_BoW = {IE_BoW}\\n')\n",
    "\n",
    "    # Computing a dictionary containing all word types\n",
    "    V = getBagOfWords(megDocGBList + megDocIEList)\n",
    "    print(f'V = {V}')\n",
    "    print(f'|V| = {len(V)}\\n')\n",
    "\n",
    "    # Iterating through all the tests\n",
    "    for test in testingSet:\n",
    "        print('-------------------------------------------------------------')\n",
    "        print(f'Test document = {test}\\n')\n",
    "        documentProbGB, documentProbIE = probGB, probIE\n",
    "        for word in test[0].split():\n",
    "            word = word.lower()\n",
    "            if word not in V:\n",
    "                continue # Ignore the word if it never appears in the training corpus\n",
    "            \n",
    "            # Computing the conditional probability for the word if it is in the + class\n",
    "            if word in GB_BoW:\n",
    "                conditionalProbGB = (GB_BoW[word] + 1)/(len(megDocGBList) + len(V))\n",
    "            else:\n",
    "                conditionalProbGB = 1/(len(megDocGBList) + len(V))\n",
    "\n",
    "            # Computing the conditional probability for the word if it is in the - class\n",
    "            if word in IE_BoW:\n",
    "                conditionalProbIE = (IE_BoW[word] + 1)/(len(megDocIEList) + len(V))\n",
    "            else:\n",
    "                conditionalProbIE = 1/(len(megDocIEList) + len(V))\n",
    "                \n",
    "            print(f'word = \"{word}\"\\t\\twordConditionalProb(+ve) = {conditionalProbGB}\\t\\twordConditionalProb(-ve) = {conditionalProbIE}')\n",
    "\n",
    "            # Multiplying to the total document probabilities for each class\n",
    "            documentProbGB *= conditionalProbGB\n",
    "            documentProbIE *= conditionalProbIE\n",
    "\n",
    "        # Printing the total document probabilities as per the classes of + and -\n",
    "        print(f'\\ndocProb(+) = {documentProbGB}\\ndocProb(-) = {documentProbIE}')\n",
    "        if documentProbGB > documentProbIE:\n",
    "            inferredClass = '+ve'\n",
    "        elif documentProbGB < documentProbIE:\n",
    "            inferredClass = '-ve'\n",
    "        else:\n",
    "            inferredClass = '+ve & -ve'\n",
    "\n",
    "        # Printing the inferred class as the result\n",
    "        print(f'Inferred class = {inferredClass}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd84d2e5-f593-4524-9be8-00f542578b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megaDoc(+ve) = very powerful the most fun film of the summer\n",
      "megaDoc(-ve) = just plain boring entirely predictable and lacks energy no surprises and very few laughs\n",
      "\n",
      "prob(+ve) = 0.4\tprob(-ve) = 0.6\n",
      "\n",
      "+ve_BoW = {'very': 1, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1}\n",
      "-ve_BoW = {'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'very': 1, 'few': 1, 'laughs': 1}\n",
      "\n",
      "V = {'very': 2, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1, 'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'few': 1, 'laughs': 1}\n",
      "|V| = 20\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Test document = ('predictable with no fun', '?')\n",
      "\n",
      "word = \"predictable\"\t\twordConditionalProb(+ve) = 0.034482758620689655\t\twordConditionalProb(-ve) = 0.058823529411764705\n",
      "word = \"no\"\t\twordConditionalProb(+ve) = 0.034482758620689655\t\twordConditionalProb(-ve) = 0.058823529411764705\n",
      "word = \"fun\"\t\twordConditionalProb(+ve) = 0.06896551724137931\t\twordConditionalProb(-ve) = 0.029411764705882353\n",
      "\n",
      "docProb(+) = 3.2801672885317154e-05\n",
      "docProb(-) = 6.106248727864848e-05\n",
      "Inferred class = -ve\n"
     ]
    }
   ],
   "source": [
    "trainingSet = [('just plain boring','-'),('entirely predictable and lacks energy','-'),('no surprises and very few laughs','-'),('very powerful','+'),('the most fun film of the summer','+')]\n",
    "testSet = [('predictable with no fun','?')]\n",
    "naiveBayesClassifier(trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b56e8-477e-4af0-8d6f-ad388c1741cc",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e19a39df-f980-45b2-bf21-f126d7640710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String = NLP is cool\n",
      "\n",
      "Sentiment(polarity=0.35, subjectivity=0.65)\n",
      "Positive sentiment ðŸ™‚\n",
      "Subjectivity: 0.65\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "String = NLP is cool and useful\n",
      "\n",
      "Sentiment(polarity=0.32499999999999996, subjectivity=0.325)\n",
      "Positive sentiment ðŸ™‚\n",
      "Subjectivity: 0.325\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "String = NLP is hard\n",
      "\n",
      "Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n",
      "Negative sentiment ðŸ˜”\n",
      "Subjectivity: 0.5416666666666666\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "String = NLP is hard and useless\n",
      "\n",
      "Sentiment(polarity=-0.39583333333333337, subjectivity=0.37083333333333335)\n",
      "Negative sentiment ðŸ˜”\n",
      "Subjectivity: 0.37083333333333335\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "String = NLP stands for Natural Language Processing\n",
      "\n",
      "Sentiment(polarity=0.1, subjectivity=0.4)\n",
      "Neutral sentiment ðŸ˜\n",
      "Subjectivity: 0.4\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentimentAnalyser(string):\n",
    "    print(f'String = {string}\\n')\n",
    "    testimonial = TextBlob(string)\n",
    "    sentimentData = testimonial.sentiment\n",
    "    print(sentimentData)\n",
    "    if sentimentData.polarity > 0.1:\n",
    "        print('Positive sentiment ðŸ™‚')\n",
    "    elif sentimentData.polarity < -0.1:\n",
    "        print('Negative sentiment ðŸ˜”')\n",
    "    else:\n",
    "        '''\n",
    "        I've kept the threshold for a positive and negative sentiment analysis to require a polarity score of more than 0.1 and less than\n",
    "        0.1 respectively instead of keeping it to a fine margin of just 0.\n",
    "        '''\n",
    "        print('Neutral sentiment ðŸ˜')\n",
    "    print(f'Subjectivity: {sentimentData.subjectivity}')\n",
    "    print('-'*80 + '\\n')\n",
    "\n",
    "sentimentAnalyser(\"NLP is cool\")\n",
    "sentimentAnalyser(\"NLP is cool and useful\")\n",
    "sentimentAnalyser(\"NLP is hard\")\n",
    "sentimentAnalyser(\"NLP is hard and useless\")\n",
    "sentimentAnalyser(\"NLP stands for Natural Language Processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
